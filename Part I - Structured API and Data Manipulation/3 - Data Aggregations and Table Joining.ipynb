{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Learning Note - Data Aggregations and Table Joining\n",
    "\n",
    "Jia Geng | gjia0214@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_252\"\r\n",
      "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~19.10-b09)\r\n",
      "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "# check java version \n",
    "# use sudo update-alternatives --config java to switch java version if needed.\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://unknown40A5EF2BBD8A:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Learning</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4ce83f7190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Spark Learning').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_example = '/home/jgeng/Documents/Git/SparkLearning/book_data/retail-data/all/online-retail-dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(data_example)\n",
    "df.printSchema()\n",
    "df.show(3)\n",
    "df.cache()  # cache is lazy operation, it does not cache data until use it\n",
    "df.count()  # since count is an action on all data, call this will cache all data on memory!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Aggregation\n",
    "\n",
    "Aggregation is to group the rows by a key and grouping function. In spark, the groupby operation will return a `RelationalGroupedDataset` object.\n",
    "\n",
    "Grouping types in spark include:\n",
    "- Dataframe level aggregation.\n",
    "- **group by**: Aggregate using one or more keys and one or more grouping functions\n",
    "- **window**: Aggregate using one or more keys and one or more grouping functions. Functions are related to the current row.\n",
    "- **group set**: Aggregate at multiple different levels\n",
    "    - **roll up**: one or more keys and one or more values, summarized hierarchically\n",
    "    - **cube**: one or more keys and one or more values, summarized across all combinations of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 DataFrame Level Aggregation\n",
    "\n",
    "Common aggregation functions on dataframes are under `pyspark.sql.functions`. Work on columns.\n",
    "- `count()`: `df.count()` is action.\n",
    "- `countDistinct()`: can be slow when data is large\n",
    "- `approx_count_distinct(col_name, prec)`: faster option, take a precision param\n",
    "- `first()`, `last()`: get first/last value of a column\n",
    "- `min()`, `max()`, `sum()`, `sumDistinct()`, `avg()`: work as it means\n",
    "- `var_pop()`, `var_sample()`, `stddev_pop()`, `stddev_sample()`: work as it means\n",
    "- `skewness()`, `kurtosis()`\n",
    "    - skewness: Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\n",
    "        - normal dist. skewness = 0 (symmetry, left/right tails are same)\n",
    "        - positive skewness: right skew - right tail is longer\n",
    "        - negative skewness: left skew - left tail is longer\n",
    "    - kurtosis: Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers. A uniform distribution would be the extreme case.\n",
    "        - normal dist. kurtosis = 0\n",
    "        - positive kurtosis: heavy tailed\n",
    "        - negative kurtosis: light tailed\n",
    "    \n",
    "- `corr()`, `covar_pop()`, `covar_sample()`\n",
    "\n",
    "Spark also support aggregate column values into an array using `collect_set()` or `collect_list()` fucntion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|DistinctCount|\n",
      "+-------------+\n",
      "|         4070|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|DistinctCount|\n",
      "+-------------+\n",
      "|         4079|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|DistinctCount|\n",
      "+-------------+\n",
      "|        45280|\n",
      "+-------------+\n",
      "\n",
      "+------+\n",
      "|Approx|\n",
      "+------+\n",
      "| 45378|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|Approx|\n",
      "+------+\n",
      "| 45314|\n",
      "+------+\n",
      "\n",
      "+---------+--------+\n",
      "|StockCode|Quantity|\n",
      "+---------+--------+\n",
      "|    21485|       6|\n",
      "|    84347|       3|\n",
      "|    22454|       2|\n",
      "+---------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45280"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, approx_count_distinct, col, struct, array\n",
    "# count, distince count, count coloumn \n",
    "df.select(countDistinct(col('StockCode')).alias('DistinctCount')).show()\n",
    "\n",
    "# work faster when data is very large\n",
    "df.select(approx_count_distinct(col('StockCode'), 0.01).alias('DistinctCount')).show()\n",
    "\n",
    "# can count distinct multiple columns\n",
    "df.select(countDistinct(col('StockCode'), col('Quantity')).alias('DistinctCount')).show()  \n",
    "\n",
    "# this would work on multiple columns but slower\n",
    "df.select(approx_count_distinct(struct(col('StockCode'), col('Quantity')), 0.01).alias('Approx')).show()\n",
    "\n",
    "# this would also work on multiple columns but slower\n",
    "df.select(approx_count_distinct(array(col('StockCode'), col('Quantity')), 0.01).alias('Approx')).show()\n",
    "\n",
    "# refresh the use of distinct() to show all distinct rows\n",
    "df.select(col('StockCode'), col('Quantity')).distinct().show(3)\n",
    "df.select(col('StockCode'), col('Quantity')).distinct().count()  # same results as countDistinct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+----+-------+----------------+------------------+-------------------+------------------+\n",
      "|   min|  max|first|last|    sum|             avg|               var|           skewness|          kurtosis|\n",
      "+------+-----+-----+----+-------+----------------+------------------+-------------------+------------------+\n",
      "|-80995|80995|    6|   3|5176450|9.55224954743324|47559.303646609165|-0.2640755761052369|119768.05495536828|\n",
      "+------+-----+-----+----+-------+----------------+------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max, first, last, sum, avg, var_pop, skewness, kurtosis\n",
    "\n",
    "# some column based stats\n",
    "min_quantity = min(df.Quantity)\n",
    "max_quantity = max(df.Quantity)\n",
    "first_quantity = first(df.Quantity)\n",
    "last_quantity = last(df.Quantity)\n",
    "sum_quantity = sum(df.Quantity)\n",
    "avg_quantity = avg(df.Quantity)\n",
    "var_quantity = var_pop(df.Quantity)\n",
    "skewness_quantity = skewness(df.Quantity)\n",
    "kurtosis_quantity = kurtosis(df.Quantity)\n",
    "\n",
    "df.select(min_quantity.alias('min'), max_quantity.alias('max'), \n",
    "          first_quantity.alias('first'), last_quantity.alias('last'),\n",
    "          sum_quantity.alias('sum'), avg_quantity.alias('avg'),\n",
    "          var_quantity.alias('var'), skewness_quantity.alias('skewness'),\n",
    "          kurtosis_quantity.alias('kurtosis')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|         Correlation|         Covariance|\n",
      "+--------------------+-------------------+\n",
      "|-0.00123492454487...|-26.058713170967746|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop\n",
    "\n",
    "# correlation between two columns\n",
    "cor_qp = corr(df.Quantity, df.UnitPrice)\n",
    "\n",
    "# correlation is covariance normalized by variance (pop/sample)\n",
    "covar_qp = covar_pop(df.Quantity, df.UnitPrice)\n",
    "\n",
    "# print it out\n",
    "df.select(cor_qp.alias('Correlation'), covar_qp.alias('Covariance')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------+\n",
      "|collect_set(Quantity)|collect_list(Quantity)|\n",
      "+---------------------+----------------------+\n",
      "| [-42, 306, 256, 1...|  [6, 6, 8, 6, 6, 2...|\n",
      "+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "\n",
    "agged_df = df.agg(collect_set(col('Quantity')), collect_list(col('Quantity')))\n",
    "agged_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 GroupBy and Aggregate\n",
    "\n",
    "More common task is to perform calculation based on the groups in the data. This is usually a two stage process:\n",
    "- group by some keys: `.groupBy(col_names, ...)`, support multiple comlumns\n",
    "- aggregate by some function `.agg(func(col), ...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----+------------------+\n",
      "|StockCode|       Country|Count|               Avg|\n",
      "+---------+--------------+-----+------------------+\n",
      "|    22154|United Kingdom|  170|0.5414117647058824|\n",
      "|    22478|United Kingdom|  133|1.8110526315789475|\n",
      "|    22844|United Kingdom|  402|10.921791044776118|\n",
      "+---------+--------------+-----+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+--------------+-----+------------------+\n",
      "|StockCode|       Country|Count|               Avg|\n",
      "+---------+--------------+-----+------------------+\n",
      "|    22154|United Kingdom|  170|0.5414117647058824|\n",
      "|    22478|United Kingdom|  133|1.8110526315789475|\n",
      "|    22844|United Kingdom|  402|10.921791044776118|\n",
      "+---------+--------------+-----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, expr, col\n",
    "\n",
    "# group by can work with multiple columns\n",
    "df.groupBy('StockCode', 'Country').agg(count(col('StockCode')).alias('Count'), \n",
    "                                       avg(col('UnitPrice')).alias('Avg')).show(3)\n",
    "\n",
    "# can use expr for full string implementation\n",
    "df.groupBy('StockCode', 'Country').agg(expr('count(StockCode)').alias('Count'), \n",
    "                                       expr('avg(UnitPrice)').alias('Avg')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Window Functions\n",
    "\n",
    "A window is a specification of which rows should be used for the computation (aggregations).\n",
    "- for groupBy, each row can only go into one group\n",
    "- **for window, a row can go into multiple groups. e.g. rolling average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
