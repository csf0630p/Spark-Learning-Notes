{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Learning Note - Data Aggregations\n",
    "\n",
    "Jia Geng | gjia0214@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_252\"\r\n",
      "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~19.10-b09)\r\n",
      "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "# check java version \n",
    "# use sudo update-alternatives --config java to switch java version if needed.\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://unknown40A5EF2BBD8A:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Learning</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4ce83f7190>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Spark Learning').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_example = '/home/jgeng/Documents/Git/SparkLearning/book_data/retail-data/all/online-retail-dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(data_example)\n",
    "df.printSchema()\n",
    "df.show(3)\n",
    "df.cache()  # cache is lazy operation, it does not cache data until use it\n",
    "df.count()  # since count is an action on all data, call this will cache all data on memory!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "Aggregation is to group the rows by a key and grouping function. In spark, the groupby operation will return a `RelationalGroupedDataset` object.\n",
    "\n",
    "Grouping types in spark include:\n",
    "- Dataframe level aggregation.\n",
    "- **group by**: Aggregate using one or more keys and one or more grouping functions\n",
    "- **window**: Aggregate using one or more keys and one or more grouping functions. Functions are related to the current row.\n",
    "- **group set**: Aggregate at multiple different levels\n",
    "    - **roll up**: one or more keys and one or more values, summarized hierarchically\n",
    "    - **cube**: one or more keys and one or more values, summarized across all combinations of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DataFrame Level Aggregation\n",
    "\n",
    "Common aggregation functions on dataframes are under `pyspark.sql.functions`. Work on columns.\n",
    "- `count()`: `df.count()` is action.\n",
    "- `countDistinct()`: can be slow when data is large\n",
    "- `approx_count_distinct(col_name, prec)`: faster option, take a precision param\n",
    "- `first()`, `last()`: get first/last value of a column\n",
    "- `min()`, `max()`, `sum()`, `sumDistinct()`, `avg()`: work as it means\n",
    "- `var_pop()`, `var_sample()`, `stddev_pop()`, `stddev_sample()`: work as it means\n",
    "- `skewness()`, `kurtosis()`\n",
    "    - skewness: Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\n",
    "        - normal dist. skewness = 0 (symmetry, left/right tails are same)\n",
    "        - positive skewness: right skew - right tail is longer\n",
    "        - negative skewness: left skew - left tail is longer\n",
    "    - kurtosis: Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers. A uniform distribution would be the extreme case.\n",
    "        - normal dist. kurtosis = 0\n",
    "        - positive kurtosis: heavy tailed\n",
    "        - negative kurtosis: light tailed\n",
    "    \n",
    "- `corr()`, `covar_pop()`, `covar_sample()`\n",
    "\n",
    "Spark also support aggregate column values into an array using `collect_set()` or `collect_list()` fucntion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|DistinctCount|\n",
      "+-------------+\n",
      "|         4070|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|DistinctCount|\n",
      "+-------------+\n",
      "|         4079|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|DistinctCount|\n",
      "+-------------+\n",
      "|        45280|\n",
      "+-------------+\n",
      "\n",
      "+------+\n",
      "|Approx|\n",
      "+------+\n",
      "| 45378|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|Approx|\n",
      "+------+\n",
      "| 45314|\n",
      "+------+\n",
      "\n",
      "+---------+--------+\n",
      "|StockCode|Quantity|\n",
      "+---------+--------+\n",
      "|    21485|       6|\n",
      "|    84347|       3|\n",
      "|    22454|       2|\n",
      "+---------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45280"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, approx_count_distinct, col, struct, array\n",
    "# count, distince count, count coloumn \n",
    "df.select(countDistinct(col('StockCode')).alias('DistinctCount')).show()\n",
    "\n",
    "# work faster when data is very large\n",
    "df.select(approx_count_distinct(col('StockCode'), 0.01).alias('DistinctCount')).show()\n",
    "\n",
    "# can count distinct multiple columns\n",
    "df.select(countDistinct(col('StockCode'), col('Quantity')).alias('DistinctCount')).show()  \n",
    "\n",
    "# this would work on multiple columns but slower\n",
    "df.select(approx_count_distinct(struct(col('StockCode'), col('Quantity')), 0.01).alias('Approx')).show()\n",
    "\n",
    "# this would also work on multiple columns but slower\n",
    "df.select(approx_count_distinct(array(col('StockCode'), col('Quantity')), 0.01).alias('Approx')).show()\n",
    "\n",
    "# refresh the use of distinct() to show all distinct rows\n",
    "df.select(col('StockCode'), col('Quantity')).distinct().show(3)\n",
    "df.select(col('StockCode'), col('Quantity')).distinct().count()  # same results as countDistinct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+----+-------+----------------+------------------+-------------------+------------------+\n",
      "|   min|  max|first|last|    sum|             avg|               var|           skewness|          kurtosis|\n",
      "+------+-----+-----+----+-------+----------------+------------------+-------------------+------------------+\n",
      "|-80995|80995|    6|   3|5176450|9.55224954743324|47559.303646609165|-0.2640755761052369|119768.05495536828|\n",
      "+------+-----+-----+----+-------+----------------+------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max, first, last, sum, avg, var_pop, skewness, kurtosis\n",
    "\n",
    "# some column based stats\n",
    "min_quantity = min(df.Quantity)\n",
    "max_quantity = max(df.Quantity)\n",
    "first_quantity = first(df.Quantity)\n",
    "last_quantity = last(df.Quantity)\n",
    "sum_quantity = sum(df.Quantity)\n",
    "avg_quantity = avg(df.Quantity)\n",
    "var_quantity = var_pop(df.Quantity)\n",
    "skewness_quantity = skewness(df.Quantity)\n",
    "kurtosis_quantity = kurtosis(df.Quantity)\n",
    "\n",
    "df.select(min_quantity.alias('min'), max_quantity.alias('max'), \n",
    "          first_quantity.alias('first'), last_quantity.alias('last'),\n",
    "          sum_quantity.alias('sum'), avg_quantity.alias('avg'),\n",
    "          var_quantity.alias('var'), skewness_quantity.alias('skewness'),\n",
    "          kurtosis_quantity.alias('kurtosis')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|         Correlation|         Covariance|\n",
      "+--------------------+-------------------+\n",
      "|-0.00123492454487...|-26.058713170967746|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop\n",
    "\n",
    "# correlation between two columns\n",
    "cor_qp = corr(df.Quantity, df.UnitPrice)\n",
    "\n",
    "# correlation is covariance normalized by variance (pop/sample)\n",
    "covar_qp = covar_pop(df.Quantity, df.UnitPrice)\n",
    "\n",
    "# print it out\n",
    "df.select(cor_qp.alias('Correlation'), covar_qp.alias('Covariance')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------+\n",
      "|collect_set(Quantity)|collect_list(Quantity)|\n",
      "+---------------------+----------------------+\n",
      "| [-42, 306, 256, 1...|  [6, 6, 8, 6, 6, 2...|\n",
      "+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "\n",
    "agged_df = df.agg(collect_set(col('Quantity')), collect_list(col('Quantity')))\n",
    "agged_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GroupBy and Aggregate\n",
    "\n",
    "More common task is to perform calculation based on the groups in the data. This is usually a two stage process:\n",
    "- group by some keys: `.groupBy(col_names, ...)`, support multiple comlumns\n",
    "- aggregate by some function `.agg(func(col), ...)`, this can take multiple functions!\n",
    "\n",
    "`.groupBy()` return a `GroupedData` object which have a function `.agg()` that can recieve functions on the grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----+------------------+\n",
      "|StockCode|       Country|Count|               Avg|\n",
      "+---------+--------------+-----+------------------+\n",
      "|    22154|United Kingdom|  170|0.5414117647058824|\n",
      "|    22478|United Kingdom|  133|1.8110526315789475|\n",
      "|    22844|United Kingdom|  402|10.921791044776118|\n",
      "+---------+--------------+-----+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+--------------+-----+------------------+\n",
      "|StockCode|       Country|Count|               Avg|\n",
      "+---------+--------------+-----+------------------+\n",
      "|    22154|United Kingdom|  170|0.5414117647058824|\n",
      "|    22478|United Kingdom|  133|1.8110526315789475|\n",
      "|    22844|United Kingdom|  402|10.921791044776118|\n",
      "+---------+--------------+-----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, expr, col\n",
    "\n",
    "# group by can work with multiple columns\n",
    "df.groupBy('StockCode', 'Country').agg(count(col('StockCode')).alias('Count'), \n",
    "                                       avg(col('UnitPrice')).alias('Avg')).show(3)\n",
    "# can use expr for full string implementation\n",
    "df.groupBy('StockCode', 'Country').agg(expr('count(StockCode)').alias('Count'), \n",
    "                                       expr('avg(UnitPrice)').alias('Avg')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Window Functions\n",
    "\n",
    "A window is a specification of which rows should be used for the computation (aggregations).\n",
    "- for groupBy, each row can only go into one group\n",
    "- **for window, a row can go into multiple groups. e.g. rolling average**\n",
    "\n",
    "The pipeline for applying window function is:\n",
    "- define a window, use `Window` object under `pyspark.sql.window`\n",
    "    - use `.partitionBy(col_names, ...)` to define the partitions\n",
    "    - use `.orderBy()` to sort values within each partition\n",
    "    - use `.rowsBeteen()` to define the criteria to generate the window. E.g. `.rowsBetween(Window.unboundedPreceding, Window.currentRow)` means\n",
    "        - a window consist of all previous row -> current row (**within the same partition**)\n",
    "    - above returns a `windowSpec` object\n",
    "\n",
    "Some aggregation functions can be apply on the windowSpec object, for example:\n",
    "- `mean(col_name).over(windowSpect)`: simply use `.over()`\n",
    "- the function should have a string column name input instead of df.colname\n",
    "\n",
    "There are also window functions such as `rank`, `dense_rank`:\n",
    "- `rank().over(windowSpec)`\n",
    "- **since this is a window function, the rank is per partition not the global rank**\n",
    "\n",
    "A common window function pipeline is:\n",
    "- create the `windowSpec`: `.partition(col_names, ...) -> .orderBy() -> `.rowsBetween(start, end)``\n",
    "- apply functions on window to get the column object\n",
    "- select the dataframe using the column object (it is a good pratice to sort the dataframe using the partition criteria for display the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6| 2010-12-01|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6| 2010-12-01|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8| 2010-12-01|     2.75|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df.show(1)\n",
    "df.printSchema()\n",
    "\n",
    "# convert the InvoiceDate from datetime to date\n",
    "# to_date(col, format), format must be specified or it will not be able to recognize\n",
    "dfWithDate = df.withColumn('InvoiceDate', to_date(col('InvoiceDate'), 'MM/d/yyyy H:mm'))\n",
    "dfWithDate.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+--------------------+----+----------+\n",
      "|CustomerId|InvoiceDate|Quantity|Rolling Max Quantity|Rank|Dense Rank|\n",
      "+----------+-----------+--------+--------------------+----+----------+\n",
      "|      null| 2010-12-01|     -10|                 -10|   1|         1|\n",
      "|      null| 2010-12-01|       1|                   1|   2|         2|\n",
      "|      null| 2010-12-01|       1|                   1|   2|         2|\n",
      "+----------+-----------+--------+--------------------+----+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+-----------+--------+--------------------+----+----------+\n",
      "|CustomerId|InvoiceDate|Quantity|Rolling Max Quantity|Rank|Dense Rank|\n",
      "+----------+-----------+--------+--------------------+----+----------+\n",
      "|     12346| 2011-01-18|  -74215|              -74215|   1|         1|\n",
      "|     12346| 2011-01-18|   74215|               74215|   2|         2|\n",
      "|     12347| 2010-12-07|       3|                   3|   1|         1|\n",
      "|     12347| 2010-12-07|       4|                   4|   2|         2|\n",
      "|     12347| 2010-12-07|       4|                   4|   2|         2|\n",
      "|     12347| 2010-12-07|       4|                   4|   2|         2|\n",
      "|     12347| 2010-12-07|       4|                   4|   2|         2|\n",
      "|     12347| 2010-12-07|       4|                   4|   2|         2|\n",
      "|     12347| 2010-12-07|       4|                   4|   2|         2|\n",
      "|     12347| 2010-12-07|       4|                   4|   2|         2|\n",
      "|     12347| 2010-12-07|       6|                   6|   9|         3|\n",
      "|     12347| 2010-12-07|       6|                   6|   9|         3|\n",
      "|     12347| 2010-12-07|       6|                   6|   9|         3|\n",
      "|     12347| 2010-12-07|       6|                   6|   9|         3|\n",
      "|     12347| 2010-12-07|       6|                   6|   9|         3|\n",
      "|     12347| 2010-12-07|       6|                   6|   9|         3|\n",
      "|     12347| 2010-12-07|       6|                   6|   9|         3|\n",
      "|     12347| 2010-12-07|      12|                  12|  16|         4|\n",
      "|     12347| 2010-12-07|      12|                  12|  16|         4|\n",
      "|     12347| 2010-12-07|      12|                  12|  16|         4|\n",
      "+----------+-----------+--------+--------------------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import max, dense_rank, rank\n",
    "\n",
    "# Step 1 - create window spec\n",
    "# pipeline to define a window spec\n",
    "# define partition -> order -> define range criteria\n",
    "windowSpecPartitioned = Window.partitionBy('CustomerId', 'InvoiceDate')\n",
    "windowSpecOrdered = windowSpecPartitioned.orderBy('Quantity')\n",
    "windowSpec = windowSpecOrdered.rowsBetween(Window.unboundedPreceding, Window.currentRow)  # start, end\n",
    "\n",
    "# Step 2 - apply function over the windowSpec to get the columns\n",
    "# These are all transformations and won't execute right away\n",
    "# When we call actions and execute these queries, it will match column names with the dataframe\n",
    "rollingMaxQuantity = max(col('Quantity')).over(windowSpec)\n",
    "quanRank = rank().over(windowSpec)\n",
    "quanDenseRank = dense_rank().over(windowSpec)\n",
    "\n",
    "# Step 3 - select columns \n",
    "# make sure to remove the nulls\n",
    "dfWithDate.orderBy('CustomerId').select('CustomerId', 'InvoiceDate', 'Quantity',\n",
    "                                rollingMaxQuantity.alias('Rolling Max Quantity'),\n",
    "                                quanRank.alias('Rank'), quanDenseRank.alias('Dense Rank')).show(3)\n",
    "\n",
    "# Step 3 - select columns \n",
    "# make sure to remove the nulls\n",
    "dfWithDate.where('CustomerId is not null').orderBy('CustomerId').select('CustomerId', 'InvoiceDate', 'Quantity',\n",
    "                                                                rollingMaxQuantity.alias('Rolling Max Quantity'),\n",
    "                                                                quanRank.alias('Rank'), quanDenseRank.alias('Dense Rank')).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Group Sets\n",
    "\n",
    "Three types of group sets aggregation given (col1, col2, col3):\n",
    "- group by (col1, col2, col3)\n",
    "- roll up - hierarchically groups: (all, all, all), (col1, all, all), (col1, col2, all), (col1, col2, col3)\n",
    "- cube - all combination groups: (all, all, all), (col1, all, all), (col2, all, all), (col3, all, all), (col1, all, col3), (all, col2, col3), (col1, col2, all), (col1, col2, col3)\n",
    "\n",
    "Aggregation on multiple groups can be easily achieved by `df.groupBy(col1, col2, col3)`\n",
    "\n",
    "**Roll Up (col1, col2, col3)** -> rollup col1 on the rest of the columns, gives us 4 levels:\n",
    "- grand total\n",
    "- sub total of each (col1) group\n",
    "- sub total of each (col1, col2) group\n",
    "- subtotal of each (col1, col2, col3) group\n",
    "\n",
    "**Cube (col1, col2, col3)** -> all combination aggregation, gives us 8 level (below is not the actual order of levels, check the code example):\n",
    "- grand total\n",
    "- sub total of each (col1) group\n",
    "- sub total of each (col2) group\n",
    "- sub total of each (col3) group\n",
    "- sub total of each (col1, col2) group\n",
    "- sub total of each (col1, col3) group\n",
    "- sub total of each (col2, col3) group\n",
    "- sub total of each (col1, col2, col3) group\n",
    "\n",
    "After the group set operations. We need to query the aggregated information. When doing the `.agg()`, we can introduce a `grouping_id()` function to introduce a column that indicate the level of aggregation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|CustomerID|StockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "+----------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# groupBy, each group is defined by the (customerID, stockCode)\n",
    "dfWithDate.groupBy('CustomerID', 'StockCode').agg(sum('Quantity')).orderBy(desc('CustomerID'), desc('StockCode')).show(3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithDate.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406829"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithDateNoNull = dfWithDate.na.drop()\n",
    "dfWithDateNoNull.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+------+\n",
      "|CustomerID|StockCode|InvoiceDate| count|\n",
      "+----------+---------+-----------+------+\n",
      "|      null|     null|       null|406829|\n",
      "|     12346|     null|       null|     2|\n",
      "|     12346|    23166| 2011-01-18|     2|\n",
      "+----------+---------+-----------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "1 4373 271988\n"
     ]
    }
   ],
   "source": [
    "# rollup \n",
    "# after roll up, we can also use agg when needed\n",
    "rolledDF = dfWithDateNoNull.rollup('CustomerID', 'StockCode', 'InvoiceDate').count().orderBy('CustomerID', 'StockCode')\n",
    "\n",
    "# (null, null) is the sum over all rows\n",
    "# (12345, null) is the sum over customerID = 12345 and all stockcode \n",
    "rolledDF.show(3)\n",
    "\n",
    "# rollup on the left!!!!\n",
    "# rollup col1 on col2, col3\n",
    "# rollup col2 on col3\n",
    "col1_count = rolledDF.where('CustomerID is null').count()\n",
    "col2_count = rolledDF.where('StockCode is null').count()\n",
    "col3_count = rolledDF.where('InvoiceDate is null').count()\n",
    "\n",
    "# number of null means number of totals\n",
    "# for col1, only one grand total\n",
    "# for col2, 4372 sub-totals of each (col1, col2) pairs + 1 grand total\n",
    "# for col3, 271988 include all above + subtotals of each (col1, col2, col3) triplet\n",
    "print(col1_count, col2_count, col3_count)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+------------------+\n",
      "|CustomerID|StockCode|InvoiceDate|     avg(Quantity)|\n",
      "+----------+---------+-----------+------------------+\n",
      "|      null|    21739|       null|              2.86|\n",
      "|      null|    22576|       null|10.786259541984732|\n",
      "|      null|    22358|       null|3.8290598290598292|\n",
      "|      null|    22603| 2010-12-01| 4.333333333333333|\n",
      "|      null|    21370| 2010-12-01|               2.0|\n",
      "+----------+---------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "913656"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "# cube\n",
    "dfWithDateNoNull.cube('CustomerID', 'StockCode', 'InvoiceDate').agg(avg('Quantity')).orderBy('CustomerID').show(5)\n",
    "\n",
    "# more levels\n",
    "dfWithDateNoNull.cube('CustomerID', 'StockCode', 'InvoiceDate').agg(avg('Quantity')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|     avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "|      null|    21527|       null|    5| 2.729559748427673|          434|\n",
      "|      null|    22437|       null|    5|15.376404494382022|         2737|\n",
      "|      null|    21390| 2010-12-02|    4|              24.0|           24|\n",
      "|      null|    20992| 2010-12-01|    4|               9.0|            9|\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|    avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|      null|     null|       null|    7|12.06130339774205|      4906888|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "\n",
      "Level 0\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "|     18074|    22189| 2010-12-01|    0|          4.0|            4|\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Level 1\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "|     16098|    21832|       null|    1|         12.0|           12|\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Level 2\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|     avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "|     15260|     null| 2010-12-02|    2|14.941176470588236|          254|\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Level 3\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|    avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|     15100|     null|       null|    3|9.666666666666666|           58|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Level 4\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|    avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|      null|    22603| 2010-12-01|    4|4.333333333333333|           13|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Level 5\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "|      null|    21739|       null|    5|         2.86|          143|\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Level 6\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|     avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "|      null|     null| 2011-03-31|    6|12.923698384201078|        14397|\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Level 7\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|    avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|      null|     null|       null|    7|12.06130339774205|      4906888|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import grouping_id\n",
    "\n",
    "# higher id means higher level of aggregation\n",
    "# the highest id means the grand total\n",
    "dfCubed = dfWithDateNoNull.cube('CustomerID', 'StockCode', 'InvoiceDate').agg(grouping_id().alias('level'), avg('Quantity'), sum('Quantity'))  \n",
    "dfCubed.orderBy('CustomerID').show(4)\n",
    "\n",
    "# to get the grand total, we just need to query on the record with the highest level\n",
    "# cube will produce 8 levels, hence the highest level is 7\n",
    "dfCubed.where('level == 7').show()  # bingo!\n",
    "\n",
    "# lets check each level\n",
    "for i in range(8):\n",
    "    query = 'level == {}'.format(i)\n",
    "    print('Level {}'.format(i))\n",
    "    dfCubed.where(query).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pivot \n",
    "\n",
    "Pivot means converting a row into a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
