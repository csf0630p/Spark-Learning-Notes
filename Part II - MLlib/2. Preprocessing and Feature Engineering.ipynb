{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Learning Note - Preprocessing and Feature Engineering\n",
    "\n",
    "Jia Geng | gjia0214@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://unknown40A5EF2BBD8A:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MLExample</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f59cc03ecd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('MLExample').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the example data\n",
    "sale_path = '/home/jgeng/Documents/Git/SparkLearning/book_data/retail-data/by-day/*.csv' \n",
    "int_path = '/home/jgeng/Documents/Git/SparkLearning/book_data/simple-ml-integers'\n",
    "simple_path = '/home/jgeng/Documents/Git/SparkLearning/book_data/simple-ml'\n",
    "scale_path = '/home/jgeng/Documents/Git/SparkLearning/book_data/simple-ml-scaling'\n",
    "sales = spark.read.format('csv').option('header', True)\\\n",
    "                                .option('inferSchema', True)\\\n",
    "                                .load(sale_path).coalesce(5).where('Description is not null')\n",
    "fakeIntDF = spark.read.parquet(int_path)\n",
    "simpleDF = spark.read.json(simple_path)\n",
    "scaleDF = spark.read.parquet(scale_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|       Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "+---------+---------+------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----+----+----+\n",
      "|int1|int2|int3|\n",
      "+----+----+----+\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "+---+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show(1)\n",
    "sales.cache()\n",
    "fakeIntDF.show(1)\n",
    "simpleDF.show(1)\n",
    "scaleDF.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Formating Modelings for Different Type of Tasks\n",
    "\n",
    "Classification and Regression\n",
    "- a Double Type column for label\n",
    "- a Vector[Double] column for features\n",
    "\n",
    "Recommendation\n",
    "- a column for user\n",
    "- a column for items (movies or books)\n",
    "- a column for rating\n",
    "\n",
    "Unsupervised Learning\n",
    "- a Vector (dense/sparse) for feature\n",
    "\n",
    "Graph Analysis\n",
    "- a DataFrame of vertices \n",
    "- a DataFrame of edges\n",
    "\n",
    "Two types of feature transformation\n",
    "- **Transfomer**: convert data in a way that is not affected by input data, e.g. `Tokenizer`\n",
    "    - all transformer and some estimator for preprocessing has a `setInputCol()` and `setOutputCol` method.\n",
    "    - transformer has a `transform()` to perform the transformation\n",
    "- **Estimator** for preprocessing: convert data in a way that is affected by input data, e.g. scaling, normalizaton, `StandardScaler` the transformatin affected by input value and distribution\n",
    "    - estimator need to be `fit()` on data first, then perform transforming using the fitted object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. pyspark.ml.feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Common Transformer for Feature Transformation\n",
    "- **High Level Transformers**\n",
    "    - RFormula: good for conventionally formatted data. No need to extract values from strings or manipulate them in any way. RFormula will automatically handle categorical input by performing one-hot encoding. Numberic and labels are converted to double.\n",
    "        - need a `.fit()` process\n",
    "    - check MLlib overview for example\n",
    "- **SQL Transformer** use SQL language to transform the data\n",
    "- **VectorAssembler**: Concate the features into one big vector. A tool that will be used (directly or indirectly) in nearly every pipline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Continuous Data\n",
    "\n",
    "Common transformation for continuous data including: **scaling** and **categorizing**.\n",
    "\n",
    "Categorizer:\n",
    "- **Bucketizer**: split conitnuous feature into buckets.\n",
    "    - need to specify the buckets with a list of splitting point (include the two boundary)\n",
    "    - use float('inf') or float('-inf') if needed \n",
    "- **QuantileDiscretizer**: use equally divided quantiles to discretinize the data\n",
    "    - need to fit on data to find the quantiles\n",
    " \n",
    "Scaler:\n",
    "- **Standard Scaler**: standardize the data. \n",
    "    - support standardize the data vectors. \n",
    "    - vector in spark is a list of different features\n",
    "    - standardization is conducted on the rows of data, instead of within each vector!!!\n",
    "- **MinMaxScaler**: 0 ~ 1\n",
    "- **MaxAbsScaler**: -1 ~ 1\n",
    "- **ElementwiseProduct**: scale each value within a vector by an arbitrary value\n",
    "    - use `pyspark.ml.linalg.Vectors` `Vectors.dense()` to declare the scaling vector\n",
    "    - `.dense()` must take float input, size must match\n",
    "    - since using arbitrary scales, no `.fit()` process\n",
    "- **Normalizer**: \n",
    "    - normalize by the lp norm of the vector (not by the feature norm!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+--------+-------------------+---------+----------+--------------+--------------------+\n",
      "|InvoiceNo|StockCode|       Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|           Tokenized|\n",
      "+---------+---------+------------------+--------+-------------------+---------+----------+--------------+--------------------+\n",
      "|   580538|    23084|RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|[rabbit, night, l...|\n",
      "+---------+---------+------------------+--------+-------------------+---------+----------+--------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# tokenizer, can be applied directly on the data!!\n",
    "tkn = Tokenizer().setInputCol('Description').setOutputCol('Tokenized')\n",
    "tkn.transform(sales).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-------------+\n",
      "|int1|int2|int3|    Assembled|\n",
      "+----+----+----+-------------+\n",
      "|   4|   5|   6|[4.0,5.0,6.0]|\n",
      "+----+----+----+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vca = VectorAssembler().setInputCols(['int1', 'int2', 'int3']).setOutputCol('Assembled')\n",
    "vca.transform(fakeIntDF).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      "\n",
      "root\n",
      " |-- id: double (nullable = false)\n",
      "\n",
      "+---+------+\n",
      "| id|bucket|\n",
      "+---+------+\n",
      "|0.0|   0.0|\n",
      "|1.0|   1.0|\n",
      "|2.0|   1.0|\n",
      "|3.0|   2.0|\n",
      "|4.0|   2.0|\n",
      "+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# convert the integer to doulbe\n",
    "contDF = spark.range(20)\n",
    "contDF.show(1)\n",
    "contDF.printSchema()\n",
    "contDF = contDF.withColumn('id', expr('cast(id as double)'))\n",
    "contDF.printSchema()\n",
    "\n",
    "# bucketize\n",
    "splits = [float('-inf'), 1, 3, 10, 15, 20, float('inf')]\n",
    "bucketer = Bucketizer().setInputCol('id').setOutputCol('bucket').setSplits(splits)\n",
    "bucketer.transform(contDF).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|bucket|\n",
      "+---+------+\n",
      "|0.0|   1.0|\n",
      "|1.0|   2.0|\n",
      "+---+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "# Quantile Discretizer Need to be fit on data to find the quantiles\n",
    "qdt = QuantileDiscretizer().setInputCol('id').setOutputCol('bucket')\\\n",
    "                            .setNumBuckets(100).setRelativeError(0)\n",
    "qdt.fit(contDF).transform(contDF).show(2)  # if there are void buckets, will ignore them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------------------------+\n",
      "|id |features      |standardized                                                |\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[1.1952286093343936,0.02337622911060922,-0.5976143046671968]|\n",
      "|1  |[2.0,1.1,1.0] |[2.390457218668787,0.2571385202167014,0.5976143046671968]   |\n",
      "|0  |[1.0,0.1,-1.0]|[1.1952286093343936,0.02337622911060922,-0.5976143046671968]|\n",
      "|1  |[2.0,1.1,1.0] |[2.390457218668787,0.2571385202167014,0.5976143046671968]   |\n",
      "|1  |[3.0,10.1,3.0]|[3.5856858280031805,2.3609991401715313,1.7928429140015902]  |\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# scaler need to learn the mean and std from the data\n",
    "# so there is a fit step!!!!\n",
    "ss = StandardScaler().setInputCol('features').setOutputCol('standardized')\n",
    "stds = ss.fit(scaleDF)\n",
    "\n",
    "# standardization is across rows NOT within each vector!!!\n",
    "stds.transform(scaleDF).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------+\n",
      "|id |features      |scaled       |\n",
      "+---+--------------+-------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.0,0.0,0.0]|\n",
      "|1  |[2.0,1.1,1.0] |[0.5,0.1,0.5]|\n",
      "|0  |[1.0,0.1,-1.0]|[0.0,0.0,0.0]|\n",
      "|1  |[2.0,1.1,1.0] |[0.5,0.1,0.5]|\n",
      "|1  |[3.0,10.1,3.0]|[1.0,1.0,1.0]|\n",
      "+---+--------------+-------------+\n",
      "\n",
      "+---+--------------+-------------------------------------------------------------+\n",
      "|id |features      |scaled                                                       |\n",
      "+---+--------------+-------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.3333333333333333,0.009900990099009901,-0.3333333333333333]|\n",
      "|1  |[2.0,1.1,1.0] |[0.6666666666666666,0.10891089108910892,0.3333333333333333]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.3333333333333333,0.009900990099009901,-0.3333333333333333]|\n",
      "|1  |[2.0,1.1,1.0] |[0.6666666666666666,0.10891089108910892,0.3333333333333333]  |\n",
      "|1  |[3.0,10.1,3.0]|[1.0,1.0,1.0]                                                |\n",
      "+---+--------------+-------------------------------------------------------------+\n",
      "\n",
      "+---+--------------+--------------+\n",
      "|id |features      |scaled        |\n",
      "+---+--------------+--------------+\n",
      "|0  |[1.0,0.1,-1.0]|[1.0,0.2,-3.0]|\n",
      "|1  |[2.0,1.1,1.0] |[2.0,2.2,3.0] |\n",
      "|0  |[1.0,0.1,-1.0]|[1.0,0.2,-3.0]|\n",
      "|1  |[2.0,1.1,1.0] |[2.0,2.2,3.0] |\n",
      "|1  |[3.0,10.1,3.0]|[3.0,20.2,9.0]|\n",
      "+---+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, MaxAbsScaler, ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# min max scale -> 0 ~ 1\n",
    "mms = MinMaxScaler().setInputCol('features').setOutputCol('scaled')\n",
    "mms.fit(scaleDF).transform(scaleDF).show(20, False)\n",
    "\n",
    "# scale -> -1 ~ 1\n",
    "ma = MaxAbsScaler().setInputCol('features').setOutputCol('scaled')\n",
    "ma.fit(scaleDF).transform(scaleDF).show(20, False)\n",
    "\n",
    "# arbitrary scale\n",
    "# must delace a vector\n",
    "# must use float, size must match\n",
    "scales = Vectors.dense(1.0, 2.0, 3.0)\n",
    "eps = ElementwiseProduct().setInputCol('features').setOutputCol('scaled').setScalingVec(scales)\n",
    "eps.transform(scaleDF).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------------------------------+\n",
      "|id |features      |Normalizer_5ccbb7c91686__output                                |\n",
      "+---+--------------+---------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.47619047619047616,0.047619047619047616,-0.47619047619047616]|\n",
      "|1  |[2.0,1.1,1.0] |[0.48780487804878053,0.26829268292682934,0.24390243902439027]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.47619047619047616,0.047619047619047616,-0.47619047619047616]|\n",
      "|1  |[2.0,1.1,1.0] |[0.48780487804878053,0.26829268292682934,0.24390243902439027]  |\n",
      "|1  |[3.0,10.1,3.0]|[0.18633540372670807,0.6273291925465838,0.18633540372670807]   |\n",
      "+---+--------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# normalize each vector by the L-p norm\n",
    "# E.g. l1 norm for first vector 1 + 1 + 0.1 = 2.1  ==> 1/2.1 = 0.476\n",
    "nm = Normalizer().setInputCol('features').setP(1)\n",
    "nm.transform(scaleDF).show(30, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Transformer for Categorical Data\n",
    "\n",
    "The most common task for categorical data is indexing: converts a categorical variable in a column to numerical one that can be plug into machine learning algorithm. Some time the input need to be tokenized before such transformation.\n",
    "\n",
    "**Categorical Encoding**\n",
    "\n",
    "- **StringIndexer**: maps strings to different numerical ids\n",
    "    - need to `.fit()` on data\n",
    "    - also works on non-string column\n",
    "    - **StringIndexer can not handle unseen data by default.** The other option is to skip the row with an unseen data.\n",
    "    - encode with certain order \n",
    "\n",
    "- **IndexToString**: convert the ids back to strings.\n",
    "    - no need to specify the string column or the matching, spark will handle all the metadata\n",
    "    - if you do need to specify the labels, do this via `.setLabels()`\n",
    "\n",
    "- **VectorIndexer**: automatic detect the numerical categorical data to 0-based cateorical data.\n",
    "    - need to fit on data\n",
    "    - need to `setMaxCategories()`\n",
    "        - E.g. if set to 2, it will detect all features that have 2 or less than 2 unqiue values then convert it to 0-based indexes\n",
    "    - be careful if you have features that are not categorical but does not have much unique values\n",
    "    \n",
    "**One-Hot Encoding**\n",
    "- **OneHotEncoder**: one hot does not introduce the numerical difference between cats\n",
    "    - **one hot encoder only works on the numerical data, usually need to convert the data using StringEncoder, then apply one hot on it**\n",
    "    - thus, one hot encoder does not need the fit process\n",
    "    - spark use sparse matrix for one hot \n",
    "    - by default it will drop the last cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+-------+\n",
      "|color| lab|value1|            value2|labelID|\n",
      "+-----+----+------+------------------+-------+\n",
      "|green|good|     1|14.386294994851129|    1.0|\n",
      "| blue| bad|     8|14.386294994851129|    0.0|\n",
      "+-----+----+------+------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----+----+------+------------------+-------+\n",
      "|color| lab|value1|            value2|labelID|\n",
      "+-----+----+------+------------------+-------+\n",
      "|green|good|     1|14.386294994851129|    2.0|\n",
      "| blue| bad|     8|14.386294994851129|    4.0|\n",
      "| blue| bad|    12|14.386294994851129|    0.0|\n",
      "|green|good|    15| 38.97187133755819|    5.0|\n",
      "|green|good|    12|14.386294994851129|    0.0|\n",
      "+-----+----+------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# automatically encode the string into numerical ids\n",
    "stdIdxer = StringIndexer().setInputCol('lab').setOutputCol('labelID')\n",
    "encoded = stdIdxer.fit(simpleDF).transform(simpleDF)\n",
    "encoded.show(2)\n",
    "\n",
    "# also work with numerical column\n",
    "stdIdxer = StringIndexer().setInputCol('value1').setOutputCol('labelID')\n",
    "stdIdxer.fit(simpleDF).transform(simpleDF).show(5)\n",
    "\n",
    "\n",
    "# if want to skip the unseen data\n",
    "stdIdxer = stdIdxer.setHandleInvalid('skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+-------+----------------------------------+\n",
      "|color| lab|value1|            value2|labelID|IndexToString_8c624bf87972__output|\n",
      "+-----+----+------+------------------+-------+----------------------------------+\n",
      "|green|good|     1|14.386294994851129|    1.0|                              good|\n",
      "| blue| bad|     8|14.386294994851129|    0.0|                               bad|\n",
      "+-----+----+------+------------------+-------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----+----+------+------------------+-------+----------------------------------+\n",
      "|color| lab|value1|            value2|labelID|IndexToString_46daa60f5f9e__output|\n",
      "+-----+----+------+------------------+-------+----------------------------------+\n",
      "|green|good|     1|14.386294994851129|    1.0|                             Great|\n",
      "| blue| bad|     8|14.386294994851129|    0.0|                          Terrible|\n",
      "+-----+----+------+------------------+-------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "# covert back, original mapping was handled by spark\n",
    "idx2str = IndexToString().setInputCol('labelID')\n",
    "idx2str.transform(encoded).show(2)\n",
    "\n",
    "# covert back, original mapping was handled by spark\n",
    "idx2str = IndexToString().setInputCol('labelID').setLabels(['Terrible', 'Great'])\n",
    "idx2str.transform(encoded).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|     features|label|\n",
      "+-------------+-----+\n",
      "|[1.0,2.0,3.0]|    1|\n",
      "|[2.0,5.0,6.0]|    2|\n",
      "|[1.0,8.0,9.0]|    3|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "sample = spark.createDataFrame([([Vectors.dense([1, 2, 3]), 1]),\n",
    "                               ([Vectors.dense([2, 5, 6]), 2]),\n",
    "                               ([Vectors.dense([1, 8, 9]), 3])], ['features', 'label'])\n",
    "sample.show()\n",
    "\n",
    "# set up the vector indexer and max cats\n",
    "vidxerA = VectorIndexer().setInputCol('features').setMaxCategories(2)  # 2 cats \n",
    "vidxerB = VectorIndexer().setInputCol('features').setMaxCategories(3)  # 3 cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+----------------------------------+\n",
      "|     features|label|VectorIndexer_4f7942a7eb42__output|\n",
      "+-------------+-----+----------------------------------+\n",
      "|[1.0,2.0,3.0]|    1|                     [0.0,2.0,3.0]|\n",
      "|[2.0,5.0,6.0]|    2|                     [1.0,5.0,6.0]|\n",
      "|[1.0,8.0,9.0]|    3|                     [0.0,8.0,9.0]|\n",
      "+-------------+-----+----------------------------------+\n",
      "\n",
      "+-------------+-----+----------------------------------+\n",
      "|     features|label|VectorIndexer_6daa1678ddbc__output|\n",
      "+-------------+-----+----------------------------------+\n",
      "|[1.0,2.0,3.0]|    1|                     [0.0,0.0,0.0]|\n",
      "|[2.0,5.0,6.0]|    2|                     [1.0,1.0,1.0]|\n",
      "|[1.0,8.0,9.0]|    3|                     [0.0,2.0,2.0]|\n",
      "+-------------+-----+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A max cat = 2 so only covert the first feature\n",
    "vidxerA.fit(sample).transform(sample).show()\n",
    "\n",
    "# B max cat = 3 so converts all features to 0-based index\n",
    "vidxerB.fit(sample).transform(sample).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+-------+-------------+\n",
      "|color| lab|value1|            value2|colorId|       onehot|\n",
      "+-----+----+------+------------------+-------+-------------+\n",
      "|green|good|     1|14.386294994851129|    1.0|(3,[1],[1.0])|\n",
      "| blue| bad|     8|14.386294994851129|    2.0|(3,[2],[1.0])|\n",
      "| blue| bad|    12|14.386294994851129|    2.0|(3,[2],[1.0])|\n",
      "|green|good|    15| 38.97187133755819|    1.0|(3,[1],[1.0])|\n",
      "|green|good|    12|14.386294994851129|    1.0|(3,[1],[1.0])|\n",
      "+-----+----+------+------------------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# label encoding first\n",
    "strIdxer = StringIndexer().setInputCol('color').setOutputCol('colorId')\n",
    "encoded = strIdxer.fit(simpleDF).transform(simpleDF)\n",
    "\n",
    "# one-hot encoding on the label encoded data\n",
    "# one-hot only works on numerical data\n",
    "# must transform to numerical indexes first\n",
    "onehot = OneHotEncoder().setInputCol('colorId').setOutputCol('onehot').setDropLast(False)\n",
    "onehot.transform(encoded).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Transform Text Data\n",
    "\n",
    "**Tokenizer**\n",
    "- take a string or words seperated by space, covert them into array of words\n",
    "**RegexTokenizer**\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
