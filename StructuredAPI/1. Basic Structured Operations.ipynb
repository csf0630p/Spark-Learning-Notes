{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Learning Note - Basic Structured Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame vs Dataset\n",
    "\n",
    "- DataFrames are untyped until runtime - when you execute on it\n",
    "- Datasets are typed during the compiling time - when you declare it\n",
    "- Usually just work with DataFrames. When need strict compile-time checking, Dataset is prefered.\n",
    "- Since Python is Dynamic language, it does not support Datasets (at least for Spark2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured API Execution Steps\n",
    "\n",
    "- Write DataFrame/Dataset/SQL code\n",
    "- Spark convert code to logical plan (analyze and **optimize** the logical plan)\n",
    "- Spark transform logical plan to physical plan (**optimize** the physical plan, how to execute on cluster)\n",
    "- Spark Execute physical plan on clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "data_example_path = '/home/jgeng/Documents/Git/SparkLearning/data/2015flight.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Session\n",
    "The entry point into all functionality in Spark is the SparkSession class. To create a basic SparkSession, just use SparkSession.builder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://unknown40A5EF2BBD8A:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7bd8c02f90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a spark session locally\n",
    "spark = SparkSession.builder.appName('Spark Example').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read File into DataFrame\n",
    "Spark session can read file of differemt formats.\n",
    "- Use `.format()` to specify file format\n",
    "- `option()` provide many configurations for reading the data such as read header\n",
    "- DataFrame object has a method `printSchema()`\n",
    "\n",
    "DataType on Read:\n",
    "- `json` file contains information regarding the type of the data (but not precision). \n",
    "- data in `csv` file will be read as string if not specified\n",
    "- **the** `option('inferSchema', True)` **will enable the schema inference for reading the csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from csv file\n",
    "df = spark.read.format('csv').load(data_example_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when does not specify the header, spark will treate each row as a data record and add header\n",
    "df.show(3)\n",
    "df.printSchema() # count is in StringType!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use option to read header\n",
    "df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(data_example_path)\n",
    "df.show(3)\n",
    "df.printSchema()  # now count is in IntegerType!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# very handy method\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load File with Manual Schema\n",
    "\n",
    "When read data from file, if not specifying the schema of your data, default schema-on-read will be created for the data. **This could cause precision issue if the data in file is in different precision. In production, it is usually recommended to manually setup the schema.**\n",
    "\n",
    "`pyspark.sql.types` have all supported data types. \n",
    "\n",
    "For a list of all: https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/ch04.html\n",
    "\n",
    "Spark DataFrame Schema is defiend by StructType and a list of StructField within it.\n",
    "\n",
    "StructField:\n",
    "- name: name of the field/column\n",
    "- type: data type\n",
    "- nullable: whether null value is allowed\n",
    "- metadata: way of storing information about this column (will be used in machine learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,IntegerType,true)))\n"
     ]
    }
   ],
   "source": [
    "# a more low level representation\n",
    "print(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a schema manually - e.g. what if count is long rather than integer\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "# field name, data type, nullable\n",
    "field_1 = StructField('DEST_COUNTRY_NAME', StringType(), True)\n",
    "field_2 = StructField('ORIGIN_COUNTRY_NAME', StringType(), True)\n",
    "field_3 = StructField('count', LongType(), False)\n",
    "\n",
    "manualSchema = StructType([field_1, field_2, field_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('header', True).schema(manualSchema).load(data_example_path)\n",
    "df.show(3) \n",
    "\n",
    "# now count is long! \n",
    "# but nullable is true\n",
    "# this is because CSV format doesn't provide any tools which allow you to specify data constraints \n",
    "# so by definition reader cannot assume that input is not null and your data indeed contains nulls.\n",
    "df.printSchema()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Column, Row, and DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'new_col'>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# create colum\n",
    "col('new_col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
