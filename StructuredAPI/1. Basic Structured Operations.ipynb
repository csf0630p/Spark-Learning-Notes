{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Learning Note - Basic Structured Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame vs Dataset\n",
    "\n",
    "- DataFrames are untyped until runtime - when you execute on it\n",
    "- Datasets are typed during the compiling time - when you declare it\n",
    "- Usually just work with DataFrames. When need strict compile-time checking, Dataset is prefered.\n",
    "- Since Python is Dynamic language, it does not support Datasets (at least for Spark2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured API Execution Steps\n",
    "\n",
    "- Write DataFrame/Dataset/SQL code\n",
    "- Spark convert code to logical plan (analyze and **optimize** the logical plan)\n",
    "- Spark transform logical plan to physical plan (**optimize** the physical plan, how to execute on cluster)\n",
    "- Spark Execute physical plan on clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "data_example_path = '/home/jgeng/Documents/Git/SparkLearning/data/2015flight.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Session\n",
    "The entry point into all functionality in Spark is the SparkSession class. To create a basic SparkSession, just use SparkSession.builder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://unknown40A5EF2BBD8A:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7bd8c02f90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a spark session locally\n",
    "spark = SparkSession.builder.appName('Spark Example').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read File into DataFrame\n",
    "Spark session can read file of differemt formats.\n",
    "- Use `.format()` to specify file format\n",
    "- `option()` provide many configurations for reading the data such as read header\n",
    "- DataFrame object has a method `printSchema()`\n",
    "\n",
    "DataType on Read:\n",
    "- `json` file contains information regarding the type of the data (but not precision). \n",
    "- data in `csv` file will be read as string if not specified\n",
    "- **the** `option('inferSchema', True)` **will enable the schema inference for reading the csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from csv file\n",
    "df = spark.read.format('csv').load(data_example_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when does not specify the header, spark will treate each row as a data record and add header\n",
    "df.show(3)\n",
    "df.printSchema() # count is in StringType!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use option to read header\n",
    "df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(data_example_path)\n",
    "df.show(3)\n",
    "df.printSchema()  # now count is in IntegerType!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# very handy method\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load File with Manual Schema\n",
    "\n",
    "When read data from file, if not specifying the schema of your data, default schema-on-read will be created for the data. **This could cause precision issue if the data in file is in different precision. In production, it is usually recommended to manually setup the schema.**\n",
    "\n",
    "`pyspark.sql.types` have all supported data types. \n",
    "\n",
    "For a list of all: https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/ch04.html\n",
    "\n",
    "Spark DataFrame Schema is defiend by StructType and a list of StructField within it.\n",
    "\n",
    "StructField:\n",
    "- name: name of the field/column\n",
    "- type: data type\n",
    "- nullable: whether null value is allowed\n",
    "- metadata: way of storing information about this column (will be used in machine learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,IntegerType,true)))\n"
     ]
    }
   ],
   "source": [
    "# a more low level representation\n",
    "print(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a schema manually - e.g. what if count is long rather than integer\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "# field name, data type, nullable\n",
    "field_1 = StructField('DEST_COUNTRY_NAME', StringType(), True)\n",
    "field_2 = StructField('ORIGIN_COUNTRY_NAME', StringType(), True)\n",
    "field_3 = StructField('count', LongType(), False)\n",
    "\n",
    "manualSchema = StructType([field_1, field_2, field_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('header', True).schema(manualSchema).load(data_example_path)\n",
    "df.show(3) \n",
    "\n",
    "# now count is long! \n",
    "# but nullable is true\n",
    "# this is because CSV format doesn't provide any tools which allow you to specify data constraints \n",
    "# so by definition reader cannot assume that input is not null and your data indeed contains nulls.\n",
    "df.printSchema()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Column, Row, and Create DataFrame from Scratch \n",
    "\n",
    "col can be used in expression for `select()` method.\n",
    "\n",
    "row is data records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'new_col'>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# create an unattached column\n",
    "new_col = col('new_col')\n",
    "new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'count'>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a column from df\n",
    "df['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DEST_COUNTRY_NAME'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all column names from a df\n",
    "df.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# create a new data record\n",
    "newRow = Row('Hi', None, 2, True)\n",
    "\n",
    "# access a value from the data record\n",
    "newRow[0]  # Python will automatically convert the value to correct type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|              NYC|                MIA|    2|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame from Scratch\n",
    "# field name, data type, nullable\n",
    "field_1 = StructField('DEST_COUNTRY_NAME', StringType(), True)\n",
    "field_2 = StructField('ORIGIN_COUNTRY_NAME', StringType(), True)\n",
    "field_3 = StructField('count', LongType(), False)\n",
    "\n",
    "manualSchema = StructType([field_1, field_2, field_3])\n",
    "\n",
    "newRow = Row('NYC', 'MIA', 2)\n",
    "\n",
    "newDF = spark.createDataFrame([newRow], manualSchema)\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Select & SelectExpr & withColumn\n",
    "\n",
    "expr supports flexible expression on columns data.\n",
    "- use `expr()`\n",
    "- use `selectExpr()`\n",
    "\n",
    "for maniplating column\n",
    "- use `df.withColumn()` to add column\n",
    "- use `df.withColumnRenamed()` to rename a column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "|            Egypt|\n",
      "+-----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('DEST_COUNTRY_NAME').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|  destination|    departure|\n",
      "+-------------+-------------+\n",
      "|United States|      Romania|\n",
      "|United States|      Croatia|\n",
      "|United States|      Ireland|\n",
      "|        Egypt|United States|\n",
      "+-------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# use AS to rename the column name\n",
    "# the returned new DataFrame will have a new column name\n",
    "newdf = df.select(expr(\"DEST_COUNTRY_NAME as destination\"), \n",
    "          expr('ORIGIN_COUNTRY_NAME as departure'))\n",
    "newdf.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------+\n",
      "|  destination|    departure|invalid|\n",
      "+-------------+-------------+-------+\n",
      "|United States|      Romania|  false|\n",
      "|United States|      Croatia|  false|\n",
      "|United States|      Ireland|  false|\n",
      "|        Egypt|United States|  false|\n",
      "+-------------+-------------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use * to select all column\n",
    "# expr can take some more operations between columns to create new column\n",
    "newdf.select('*', expr('destination=departure as invalid')).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------+---+----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|invalid|one|bool|\n",
      "+-----------------+-------------------+-----+-------+---+----+\n",
      "|    United States|            Romania|   15|  false|  1|true|\n",
      "|    United States|            Croatia|    1|  false|  1|true|\n",
      "|    United States|            Ireland|  344|  false|  1|true|\n",
      "|            Egypt|      United States|   15|  false|  1|true|\n",
      "+-----------------+-------------------+-----+-------+---+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use selectExpr to do it all use one line\n",
    "# takes the expression as input, it can also take a literal as input1\n",
    "df.selectExpr('*', 'DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as invalid', '1 as one', 'True as bool').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|one|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Column<b'count[0]'>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# a more formal way to add column is to use withColumn\n",
    "df.withColumn('one', lit(1)).show(2)  # this is NOT inplace, scala never inplace change table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|invalid|\n",
      "+-----------------+-------------------+-----+-------+\n",
      "|    United States|            Romania|   15|  false|\n",
      "|    United States|            Croatia|    1|  false|\n",
      "+-----------------+-------------------+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withcolumn also support expr\n",
    "df.withColumn('invalid', expr('DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|  destination|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|United States|\n",
      "|    United States|            Croatia|    1|United States|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumn can also be used for copy and rename a column\n",
    "df.withColumn('destination', expr('DEST_COUNTRY_NAME')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----+\n",
      "|         dest|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------+-------------------+-----+\n",
      "|United States|            Romania|   15|\n",
      "|United States|            Croatia|    1|\n",
      "+-------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a more straight forward way to rename a column is \n",
    "df.withColumnRenamed('DEST_COUNTRY_NAME', 'dest').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
